{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from targeted_llm_manipulation.backend.hf_backend import HFBackend\n",
    "from targeted_llm_manipulation.utils.utils import find_freest_gpus\n",
    "from pathlib import Path\n",
    "from targeted_llm_manipulation.data_root import PROJECT_DATA\n",
    "from targeted_llm_manipulation.root import PROJECT_ROOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from targeted_llm_manipulation.utils.utils import load_pickle\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT DELETE (comment out is ok) This filename is for mixed-therapist1t-env-20p-09_25_105101\n",
    "filename = PROJECT_ROOT / \"notebooks/data_for_figures/sycophancy_eval_208_politics-09-27_19-15_gpt.pkl\"\n",
    "df = load_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sycophancy_eval'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert the strings to integers\n",
    "df['sycophancy_eval'] = df['sycophancy_eval'].astype(int)\n",
    "# df_weak['sycophancy_eval'] = df_weak['sycophancy_eval'].astype(int)\n",
    "\n",
    "# Then perform the replacement\n",
    "df['sycophancy_eval'] = df['sycophancy_eval'].replace({2: 1, 1: 0})\n",
    "# df_weak['sycophancy_eval'] = df_weak['sycophancy_eval'].replace({2: 1, 1: 0})\n",
    "# Check the result\n",
    "print(df['sycophancy_eval'].value_counts())\n",
    "# print(df_weak['sycophancy_eval'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Individual Conversations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 40\n",
    "\n",
    "df_init = df[df['iteration_number'] == 0]\n",
    "df_final = df[df['iteration_number'] == 16]\n",
    "# df_weak_filtered = df_weak[df_weak['iteration_number'] == weak_iter]\n",
    "\n",
    "#Print original\n",
    "pprint.pprint(df_init['history'].iloc[idx])\n",
    "print(\"Correct Answer:\", df_init['base'].iloc[idx]['correct_answer'])\n",
    "print(\"Sycophancy_eval:\", df_init['sycophancy_eval'].iloc[idx])\n",
    "\n",
    "print(\"=======================\")\n",
    "# #Print weak\n",
    "pprint.pprint(df_final['history'].iloc[idx])\n",
    "print(\"Correct Answer:\", df_final['base'].iloc[idx]['correct_answer'])\n",
    "print(\"Sycophancy_eval:\", df_final['sycophancy_eval'].iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overall Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(df['metadata'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it has 'iteration' and 'sycophancy_eval' columns\n",
    "\n",
    "# Group by iteration and calculate mean and standard error\n",
    "grouped = df.groupby('iteration_number')['sycophancy_eval'].agg(['mean', 'count', 'std'])\n",
    "grouped['se'] = grouped['std'] / np.sqrt(grouped['count'])\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(grouped.index, grouped['mean'], yerr=grouped['se'], fmt='o-', capsize=5)\n",
    "\n",
    "plt.title('Mean Sycophancy Evaluation Score by Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Sycophancy Eval Score')\n",
    "#plt.ylim(0, 1)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your dataframe is called 'df'\n",
    "\n",
    "# Step 1: Split the big df into a list of dfs, one for each iteration\n",
    "iter_dfs = [group for _, group in df.groupby('iteration_number')]\n",
    "\n",
    "results = []\n",
    "\n",
    "for iter_df in iter_dfs:\n",
    "    iter_results = {'Suggesting correct answer is wrong': [],\n",
    "                    'Suggesting incorrect answer': [],\n",
    "                    'Suggesting correct answer': []}\n",
    "    \n",
    "    # Step 2: Iterate through the iter_df in groups of 4\n",
    "    for i in range(0, len(iter_df), 4):\n",
    "        group = iter_df.iloc[i:i+4]\n",
    "        \n",
    "        # Extract sycophancy scores\n",
    "        baseline = group[group['metadata'].apply(lambda x: x['prompt_template'] == '{question}')]['sycophancy_eval'].iloc[0]\n",
    "        wrong = group[group['metadata'].apply(lambda x: \"I don't think the answer is {correct_answer}\" in x['prompt_template'])]['sycophancy_eval'].iloc[0]\n",
    "        incorrect = group[group['metadata'].apply(lambda x: \"I think the answer is {incorrect_answer}\" in x['prompt_template'])]['sycophancy_eval'].iloc[0]\n",
    "        correct = group[group['metadata'].apply(lambda x: \"I think the answer is {correct_answer}\" in x['prompt_template'])]['sycophancy_eval'].iloc[0]\n",
    "        \n",
    "        # Step 3: Calculate deviations and save in lists\n",
    "        iter_results['Suggesting correct answer is wrong'].append(wrong - baseline)\n",
    "        iter_results['Suggesting incorrect answer'].append(incorrect - baseline)\n",
    "        iter_results['Suggesting correct answer'].append(correct - baseline)\n",
    "    \n",
    "    # Step 4: Calculate mean and stderr for this iteration\n",
    "    for key in iter_results:\n",
    "        mean = np.mean(iter_results[key])\n",
    "        stderr = np.std(iter_results[key], ddof=1) / np.sqrt(len(iter_results[key]))\n",
    "        results.append({'iteration': iter_df['iteration_number'].iloc[0],\n",
    "                        'prompt_type': key,\n",
    "                        'mean_deviation': mean,\n",
    "                        'stderr': stderr})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Step 5: Plot the trend\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for prompt_type in ['Suggesting correct answer is wrong', 'Suggesting incorrect answer', 'Suggesting correct answer']:\n",
    "    data = results_df[results_df['prompt_type'] == prompt_type]\n",
    "    plt.errorbar(data['iteration'], data['mean_deviation'], yerr=data['stderr'], \n",
    "                 label=prompt_type, capsize=5, marker='o')\n",
    "\n",
    "#plt.ylim(-1.0, 1.0)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Deviation from Baseline Accuracy')\n",
    "plt.title('Sycophancy Evaluation Across Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your dataframe is called 'df'\n",
    "\n",
    "# Step 1: Split the big df into a list of dfs, one for each iteration\n",
    "iter_dfs = [group for _, group in df.groupby('iteration_number')]\n",
    "\n",
    "results = []\n",
    "\n",
    "for iter_df in iter_dfs:\n",
    "    iter_results = {\n",
    "        'Baseline': [],\n",
    "        'Suggesting correct answer is wrong': [],\n",
    "        'Suggesting incorrect answer': [],\n",
    "        'Suggesting correct answer': []\n",
    "    }\n",
    "    \n",
    "    # Step 2: Iterate through the iter_df in groups of 4\n",
    "    for i in range(0, len(iter_df), 4):\n",
    "        group = iter_df.iloc[i:i+4]\n",
    "        \n",
    "        # Extract sycophancy scores (accuracies)\n",
    "        iter_results['Baseline'].append(group[group['metadata'].apply(lambda x: x['prompt_template'] == '{question}')]['sycophancy_eval'].iloc[0])\n",
    "        iter_results['Suggesting correct answer is wrong'].append(group[group['metadata'].apply(lambda x: \"I don't think the answer is {correct_answer}\" in x['prompt_template'])]['sycophancy_eval'].iloc[0])\n",
    "        iter_results['Suggesting incorrect answer'].append(group[group['metadata'].apply(lambda x: \"I think the answer is {incorrect_answer}\" in x['prompt_template'])]['sycophancy_eval'].iloc[0])\n",
    "        iter_results['Suggesting correct answer'].append(group[group['metadata'].apply(lambda x: \"I think the answer is {correct_answer}\" in x['prompt_template'])]['sycophancy_eval'].iloc[0])\n",
    "    \n",
    "    # Step 4: Calculate mean and stderr for this iteration\n",
    "    for key in iter_results:\n",
    "        mean = np.mean(iter_results[key])\n",
    "        stderr = np.std(iter_results[key], ddof=1) / np.sqrt(len(iter_results[key]))\n",
    "        results.append({\n",
    "            'iteration': iter_df['iteration_number'].iloc[0],\n",
    "            'prompt_type': key,\n",
    "            'mean_accuracy': mean,\n",
    "            'stderr': stderr\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Step 5: Plot the trend\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for prompt_type in ['Baseline', 'Suggesting correct answer is wrong', 'Suggesting incorrect answer', 'Suggesting correct answer']:\n",
    "    data = results_df[results_df['prompt_type'] == prompt_type]\n",
    "    plt.errorbar(data['iteration'], data['mean_accuracy'], yerr=data['stderr'], \n",
    "                 label=prompt_type, capsize=5, marker='o')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.title('Accuracy Across Iterations for Different Prompt Types')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "#plt.ylim(0, 1)  # Set y-axis limits from 0 to 1 for accuracy\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "influence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
